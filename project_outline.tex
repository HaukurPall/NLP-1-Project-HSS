
1. Make  work KenLM
Get to work with our corpus
See: http://thegrandjanitor.com/2015/12/28/using-arpa-lm-with-python/
Finished by: 12th of Nov

2. FF (Bengio)
Word embeddings
http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
-Have perplexity measurement working
-19th of Nov

3. RAN
https://arxiv.org/abs/1705.07393
Using the same weight matrices for the embeddings in the output and input. Should give higher perplexity. Why?

Idea:
RAN should handle histories better. We might want to measure this.
Do FF handle P(word) better than the RAN?
